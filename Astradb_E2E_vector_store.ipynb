{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrgOhk8U4Rpl"
      },
      "source": [
        "# Querying PDF With Astra database and LangChain\n",
        "\n",
        "### A question-answering demo using Astra DB and LangChain, powered by Vector Search\n",
        "\n",
        "This notebook demonstrates how to perform semantic search and question-answering over your own PDF documents using [Astra DB](https://astra.datastax.com) with Vector Search and [LangChain](https://python.langchain.com/). You'll learn how to:\n",
        "\n",
        "- Extract text from a PDF file using PyPDF2.\n",
        "- Split the text into manageable chunks for embedding.\n",
        "- Generate vector embeddings with OpenAI and store them in Astra DB.\n",
        "- Use LangChain to perform semantic search and retrieve relevant document excerpts.\n",
        "- Interactively ask questions about your PDF and get answers powered by large language models.\n",
        "\n",
        "**Requirements:**  \n",
        "- Astra DB account with a serverless database enabled for vector search  \n",
        "- OpenAI API key  \n",
        "- A PDF file to analyze\n",
        "\n",
        "Follow the steps in this notebook to set up your environment, connect to Astra DB, process your PDF, and start querying your documents with natural language!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqfJKgRM4Rpo"
      },
      "source": [
        "#### Pre-requisites\n",
        "\n",
        "- Astra DB account with a serverless database enabled for vector search\n",
        "- OpenAI API key\n",
        "- A PDF file to analyze (e.g., `budget_speech.pdf`)\n",
        "\n",
        "#### Steps\n",
        "\n",
        "1. Install required Python packages.\n",
        "2. Enter your Astra DB credentials and OpenAI API key.\n",
        "3. Import dependencies.\n",
        "4. Load and extract text from your PDF.\n",
        "5. Split the text into chunks.\n",
        "6. Generate embeddings and store them in Astra DB.\n",
        "7. Ask questions about your PDF and get answers using vector search and LLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_FeN-Ep4Rpp"
      },
      "source": [
        "Install the required dependencies:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQQN-L2J4Rpq"
      },
      "source": [
        "Import the packages you'll need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\rroha\\AppData\\Local\\Temp\\ipykernel_32156\\650696219.py:37: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\rroha\\OneDrive\\Desktop\\Raj\\Gen_AI_Learning_Raj\\Generative_AI_Venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rroha\\OneDrive\\Desktop\\Raj\\Gen_AI_Learning_Raj\\Generative_AI_Venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rroha\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inserted 50 headlines.\n"
          ]
        }
      ],
      "source": [
        "# Import LangChain components for vector store, index wrapper, LLM, and embeddings\n",
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Import Hugging Face dataset loader for optional dataset retrieval\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Import CassIO for Astra DB integration and PyPDF2 for PDF reading\n",
        "import cassio\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Groq and Astra DB keys\n",
        "GROQ_API_KEY = \"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\"\n",
        "ASTRA_DB_APPLICATION_TOKEN = \"AstraCS:vCwxHYfhNjZQYDpsWroJxZvz:fa3914d1e5d696abd142d29aae29008e1c8113621dbed09dc4b1143f31f2bcf8\" \n",
        "ASTRA_DB_ID = \"3b0814ca-1f8e-4458-8a17-f0986718b395\"\n",
        "\n",
        "# Specify the path to your PDF file\n",
        "pdf_path = 'attention.pdf'\n",
        "\n",
        "# Initialize the PDF reader\n",
        "pdfreader = PdfReader(pdf_path)\n",
        "\n",
        "# Extract text from all pages in the PDF\n",
        "raw_text = ''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "    content = page.extract_text()\n",
        "    if content:\n",
        "        raw_text += content\n",
        "\n",
        "# Initialize CassIO with Astra DB credentials\n",
        "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)\n",
        "\n",
        "# Set up Groq LLM and Hugging Face embedding model\n",
        "llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=\"llama3-8b-8192\")\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# Create the Astra vector store for storing embeddings\n",
        "astra_vector_store = Cassandra(\n",
        "    embedding=embedding,\n",
        "    table_name=\"qa_mini_demo\",\n",
        "    session=None,\n",
        "    keyspace=None,\n",
        ")\n",
        "\n",
        "# Import text splitter for chunking the PDF text\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)\n",
        "\n",
        "# Add the first 50 text chunks to the Astra vector store\n",
        "astra_vector_store.add_texts(texts[:50])\n",
        "\n",
        "print(\"Inserted %i headlines.\" % len(texts[:50]))\n",
        "\n",
        "# Wrap the vector store with a LangChain index for querying\n",
        "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above workflow demonstrates a complete Retrieval-Augmented Generation (RAG) pipeline using free and efficient alternatives to OpenAI: Groq as the LLM and Hugging Face for embeddings. It starts by importing essential LangChain components and reading a local PDF using PyPDF2. The extracted content is split into manageable chunks using CharacterTextSplitter to optimize embedding quality. CassIO initializes integration with AstraDB, a cloud-native vector database, using secure credentials. Instead of OpenAI, the script leverages Groq's blazing-fast LLaMA3 model (llama3-8b-8192) via ChatGroq to serve as the reasoning engine. For embeddings, it switches to BAAI/bge-small-en-v1.5, a well-performing open-source model via HuggingFaceEmbeddings, eliminating the need for paid APIs. These chunks are embedded and stored in a Cassandra-based vector store hosted on AstraDB using langchain.vectorstores.cassandra. Finally, the store is wrapped with VectorStoreIndexWrapper, enabling semantic search and natural language querying powered by Groq. This setup offers a scalable, low-cost alternative to proprietary solutions and lays the foundation for building personalized AI apps, such as document summarizers or question-answering interfaces.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code block implements an interactive question-answering loop that leverages LangChain’s VectorStoreIndexWrapper along with Groq’s LLaMA3 model to respond to natural language queries over embedded PDF data stored in AstraDB. It begins by prompting the user to enter a question and continues the conversation until \"quit\" is typed. Each query is processed by the Groq-powered LLM using stored document chunks for context-aware answers. Additionally, the loop performs a similarity search using Hugging Face embeddings to fetch the top four most relevant text snippets from the PDF, along with their similarity scores, offering insight into the sources behind each answer. This setup completes a lightweight, real-time retrieval-augmented generation (RAG) pipeline using entirely free and open tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbJugrh7SX3C",
        "outputId": "10e8f954-a113-47a2-a84c-615a9f6e5dc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "QUESTION: \"What is supervise machine learning ?\"\n",
            "ANSWER: \"Supervised machine learning is a type of machine learning where the algorithm is trained on labeled data, meaning the data has been pre-classified or pre-labeled with the correct output or response. The goal of supervised learning is to train the algorithm to predict the output or response for new, unseen data based on the patterns and relationships learned from the labeled data.\n",
            "\n",
            "In other words, supervised learning involves learning from a dataset where the correct output is already known, and the algorithm is trained to predict the correct output for new, unseen data. This is in contrast to unsupervised learning, where the algorithm is trained on unlabeled data and must discover patterns and relationships on its own.\n",
            "\n",
            "Some common examples of supervised learning tasks include:\n",
            "\n",
            "* Image classification: training an algorithm to classify images as cats or dogs\n",
            "* Sentiment analysis: training an algorithm to classify text as positive, negative, or neutral\n",
            "* Speech recognition: training an algorithm to recognize spoken words and phrases\n",
            "\n",
            "In the context of the provided text, the authors mention training their models on a dataset of sentence pairs, which suggests that they are using supervised learning.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n",
            "    [0.8140] \"batch contained a set of sentence pairs containing approximately 25000 source tokens ...\"\n",
            "    [0.8128] \"and semantic structure of the sentences. 5 Training This section describes the train ...\"\n",
            "    [0.8125] \"Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1 Transformer (4 layers) semi ...\"\n",
            "    [0.8074] \"Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supe ...\"\n",
            "\n",
            "QUESTION: \"Explain best deep learning models for classification problems.\"\n",
            "ANSWER: \"I don't know the best deep learning models for classification problems. However, some popular and widely used deep learning models for classification problems include:\n",
            "\n",
            "1. Convolutional Neural Networks (CNNs): These models are commonly used for image classification tasks, but can also be used for other types of data.\n",
            "2. Recurrent Neural Networks (RNNs): These models are commonly used for sequential data such as text or time series data.\n",
            "3. Long Short-Term Memory (LSTM) Networks: These are a type of RNN that are commonly used for sequential data.\n",
            "4. Transformers: These models are commonly used for natural language processing tasks such as language translation and text classification.\n",
            "5. Deep Neural Networks (DNNs): These models are commonly used for a wide range of classification tasks, including image classification, speech recognition, and text classification.\n",
            "\n",
            "It's important to note that the choice of model will depend on the specific problem you're trying to solve and the characteristics of your data.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n",
            "    [0.8827] \"recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease t ...\"\n",
            "    [0.8764] \"The dimensionality of input and output is dmodel = 512 , and the inner-layer has dim ...\"\n",
            "    [0.8624] \"ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [18] 23.75 Deep-Att  ...\"\n",
            "    [0.8618] \"attention is 0.9 BLEU worse than the best setting, quality also drops off with too m ...\"\n"
          ]
        }
      ],
      "source": [
        "first_question = True\n",
        "while True:\n",
        "    prompt = (\n",
        "        \"\\nEnter your question (or type 'quit' to exit): \"\n",
        "        if first_question else\n",
        "        \"\\nWhat's your next question (or type 'quit' to exit): \"\n",
        "    )\n",
        "    query_text = input(prompt).strip()\n",
        "\n",
        "    if query_text.lower() == \"quit\":\n",
        "        break\n",
        "    if not query_text:\n",
        "        continue\n",
        "\n",
        "    first_question = False\n",
        "\n",
        "    print(f'\\nQUESTION: \"{query_text}\"')\n",
        "    answer = astra_vector_index.query(query_text, llm=llm).strip()\n",
        "    print(f'ANSWER: \"{answer}\"\\n')\n",
        "\n",
        "    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
        "    for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):\n",
        "        snippet = doc.page_content[:84].replace(\"\\n\", \" \")\n",
        "        print(f'    [{score:.4f}] \"{snippet} ...\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interview Questions and Answers Based on the Workflow\n",
        "\n",
        "1. **What is Retrieval-Augmented Generation (RAG) and how is it implemented in this notebook?**  \n",
        "    *RAG combines information retrieval with generative models to answer questions using both retrieved context and language generation. Here, text is extracted from a PDF, chunked, embedded, stored in Astra DB, and relevant chunks are retrieved to provide context for Groq’s LLaMA3 model to generate answers.*\n",
        "\n",
        "2. **How does Astra DB enable semantic search, and why is it suitable for document retrieval?**  \n",
        "    *Astra DB stores high-dimensional embeddings and supports similarity queries using vector distance metrics, enabling semantic search that matches queries to document chunks based on meaning rather than keywords.*\n",
        "\n",
        "3. **Explain the role of Hugging Face embeddings in this pipeline.**  \n",
        "    *Hugging Face embeddings convert text chunks into numerical vectors that capture semantic meaning, which are then stored and used for similarity search in Astra DB.*\n",
        "\n",
        "4. **What are the advantages of using Groq’s LLaMA3 model via ChatGroq?**  \n",
        "    *Groq’s LLaMA3 is open-source, cost-effective, and accessible without proprietary APIs, reducing dependency on paid services and offering flexibility.*\n",
        "\n",
        "5. **Describe the process and importance of chunking text from a PDF for embedding.**  \n",
        "    *Text is extracted from the PDF and split into manageable chunks to fit embedding model and database limits. Chunking improves retrieval granularity and embedding quality.*\n",
        "\n",
        "6. **How does LangChain facilitate integration between vector stores and LLMs for question answering?**  \n",
        "    *LangChain provides abstractions to connect vector stores (like Astra DB) with LLMs, handling retrieval and passing relevant context to the LLM for answer generation.*\n",
        "\n",
        "7. **What security practices should be followed when handling API keys and credentials in notebooks?**  \n",
        "    *Use environment variables or secret management tools for sensitive information, avoid hardcoding, and restrict notebook access to authorized users.*\n",
        "\n",
        "8. **How does the similarity search mechanism work, and what does it provide to the user?**  \n",
        "    *It compares the embedding of the user’s query with stored document embeddings to find the most relevant chunks, displaying top matches and their similarity scores.*\n",
        "\n",
        "9. **What are the potential limitations of using open-source embeddings and LLMs?**  \n",
        "    *They may have lower accuracy or fewer features than proprietary solutions, but offer transparency, flexibility, and cost savings.*\n",
        "\n",
        "10. **How can this workflow be extended to support multiple documents or real-time updates?**  \n",
        "     *Assign unique identifiers to each document and store their embeddings separately. For real-time updates, monitor for changes and update the vector store as needed.*\n",
        "\n",
        "11. **Why is chunk overlap important when splitting text for embeddings, and how is it configured here?**  \n",
        "     *Chunk overlap preserves context at chunk boundaries, reducing loss of meaning. Here, it’s set to 200 characters.*\n",
        "\n",
        "12. **What challenges might arise when extracting text from PDFs, and how can they be addressed?**  \n",
        "     *Complex layouts or encodings can hinder extraction. Use robust libraries like PyPDF2, and for more complex cases, consider preprocessing or OCR tools.*\n",
        "\n",
        "13. **How does the workflow ensure only the most relevant document chunks are used for answering a query?**  \n",
        "     *The similarity search retrieves the top-k most relevant chunks based on vector similarity, and only these are provided as context to the LLM.*\n",
        "\n",
        "14. **How could you adapt this workflow to support other file formats, such as Word or plain text files?**  \n",
        "     *Replace the PDF extraction step with appropriate libraries (e.g., python-docx for Word, or direct reading for text files), then follow the same chunking, embedding, and storage process.*\n",
        "\n",
        "15. **Suppose I am a data scientist with many Excel files containing datasets. How can I use this workflow to gain insights from them?**  \n",
        "     *You can use pandas to read Excel files, extract and preprocess the relevant text or data, chunk the content as needed, and then embed and store it in Astra DB. For example:*\n",
        "\n",
        "     ```python\n",
        "     import pandas as pd\n",
        "     df = pd.read_excel('example.xlsx')\n",
        "     text_data = df.to_csv(index=False)  # Convert data to text\n",
        "     # Then chunk, embed, and store as with PDF text\n",
        "     ```\n",
        "     *This allows you to semantically search and query across large collections of Excel datasets for insights.*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
